Assigned,Name ,Type,Abstract,Notes,URL,How dataset constructed,Activities defined,How are activities marked,Any data cleaning,Other data relevant to activity recognition,Misc notes
,The Cadaver in the Machine: The Social Practices of Measurement and Validation in Motion Capture Technology ,Socio-Techinical ,"Motion capture systems, used across various domains, make body representations concrete through technical processes. We argue that the measurement of bodies and the validation of measurements for motion capture systems can be understood as social practices. By analyzing the findings of a systematic literature review (N=278) through the lens of social practice theory, we show how these practices, and their varying attention to errors, become ingrained in motion capture design and innovation over time. Moreover, we show how contemporary motion capture systems perpetuate assumptions about human bodies and their movements. We suggest that social practices of measurement and validation are ubiquitous in the development of data- and sensor-driven systems more broadly, and provide this work as a basis for investigating hidden design assumptions and their potential negative consequences in human-computer interaction. ",,https://arxiv.org/abs/2401.10877 ,,,,,,
,Deep Learning-based Human Pose Estimation: A Survey,Techinical ,"Human pose estimation aims to locate the human body parts and build human body representation (e.g., body skeleton) from input data such as images and videos. It has drawn increasing attention during the past decade and has been utilized in a wide range of applications including human-computer interaction, motion analysis, augmented reality, and virtual reality. Although the recently developed deep learning-based solutions have achieved high performance in human pose estimation, there still remain challenges due to insufficient training data, depth ambiguities, and occlusion. The goal of this survey article is to provide a comprehensive review of recent deep learning-based solutions for both 2D and 3D pose estimation via a systematic analysis and comparison of these solutions based on their input data and inference procedures. More than 260 research papers since 2014 are covered in this survey. Furthermore, 2D and 3D human pose estimation datasets and evaluation metrics are included. Quantitative performance comparisons of the reviewed methods on popular datasets are summarized and discussed. Finally, the challenges involved, applications, and future research directions are concluded. A regularly updated project page is provided: https://github.com/zczcwh/DL-HPE. ",Paywall,https://dl.acm.org/doi/abs/10.1145/3603618?casa_token=dnIVlU7q4QkAAAAA%3AyFWcpbjMTUkAGac1DwIlCdn6bHPkOMLCZFUKQLdYsH3Bp_MJwDmsgEbxzFd_Ig2SW_RR1euu7toxXg,,,,,,
,EfficientHRNet: Efficient Scaling for Lightweight High-Resolution Multi-Person Pose Estimation ,Techinical ,"There is an increasing demand for lightweight multi-person pose estimation for many emerging smart IoT applications. However, the existing algorithms tend to have large model sizes and intense computational requirements, making them ill-suited for real-time applications and deployment on resource-constrained hardware. Lightweight and real-time approaches are exceedingly rare and come at the cost of inferior accuracy. In this paper, we present EfficientHRNet, a family of lightweight multi-person human pose estimators that are able to perform in real-time on resource-constrained devices. By unifying recent advances in model scaling with high-resolution feature representations, EfficientHRNet creates highly accurate models while reducing computation enough to achieve real-time performance. The largest model is able to come within 4.4% accuracy of the current state-of-the-art, while having 1/3 the model size and 1/6 the computation, achieving 23 FPS on Nvidia Jetson Xavier. Compared to the top real-time approach, EfficientHRNet increases accuracy by 22% while achieving similar FPS with 1/3 the power. At every level, EfficientHRNet proves to be more computationally efficient than other bottom-up 2D human pose estimation approaches, while achieving highly competitive accuracy. ",,https://arxiv.org/abs/2007.08090,,,,,,
,TFPose: Direct Human Pose Estimation with Transformers ,Techinical ,"We propose a human pose estimation framework that solves the task in the regression-based fashion. Unlike previous regression-based methods, which often fall behind those state-of-the-art methods, we formulate the pose estimation task into a sequence prediction problem that can effectively be solved by transformers. Our framework is simple and direct, bypassing the drawbacks of the heatmap-based pose estimation. Moreover, with the attention mechanism in transformers, our proposed framework is able to adaptively attend to the features most relevant to the target keypoints, which largely overcomes the feature misalignment issue of previous regression-based methods and considerably improves the performance. Importantly, our framework can inherently take advantages of the structured relationship between keypoints. Experiments on the MS-COCO and MPII datasets demonstrate that our method can significantly improve the state-of-the-art of regression-based pose estimation and perform comparably with the best heatmap-based pose estimation methods. ",,https://arxiv.org/abs/2103.15320,,,,,,
,3D Human Pose Estimation with Spatial and Temporal Transformer ,Techinical ,"Transformer architectures have become the model of choice in natural language processing and are now being introduced into computer vision tasks such as image classification, object detection, and semantic segmentation. However, in the field of human pose estimation, convolutional architectures still remain dominant. In this work, we present PoseFormer, a purely transformer-based approach for 3D human pose estimation in videos without convolutional architectures involved. Inspired by recent developments in vision transformers, we design a spatial-temporal transformer structure to comprehensively model the human joint relations within each frame as well as the temporal correlations across frames, then output an accurate 3D human pose of the center frame. We quantitatively and qualitatively evaluate our method on two popular and standard benchmark datasets: Human3.6M and MPI-INF-3DHP. Extensive experiments show that PoseFormer achieves state-ofthe-art performance on both datasets. Code is available at https://github.com/zczcwh/PoseFormer ",,https://arxiv.org/pdf/2103.10455.pdf,,,,,,
,Human3.6M: Large Scale Datasets and Predictive Methods for 3D Human Sensing in Natural Environments,Dataset,"We introduce a new dataset, Human3.6M, of 3.6 Million accurate 3D Human poses, acquired by recording the performance of 5 female and 6 male subjects, under 4 different viewpoints, for training realistic human sensing systems and for evaluating the next generation of human pose estimation models and algorithms. Besides increasing the size of the datasets in the current state of the art by several orders of magnitude, we also aim to complement such datasets with a diverse set of motions and poses encountered as part of typical human activities (taking photos, talking on the phone, posing, greeting, eating, etc.), with additional synchronized image, human motion capture and time of flight (depth) data, and with accurate 3D body scans of all the subject actors involved. We also provide controlled mixed reality evaluation scenarios where 3D human models are animated using motion capture and inserted using correct 3D geometry, in complex real environments, viewed with moving cameras, and under occlusion. Finally, we provide a set of large scale statistical models and detailed evaluation baselines for the dataset illustrating its diversity and the scope for improvement by future work in the research community. Our experiments show that our best large scale model can leverage our full training set to obtain a 20% improvement in performance compared to a training set of the scale of the largest existing public dataset for this problem. Yet the potential for improvement by leveraging higher capacity, more complex models with our large dataset, is substantially vaster and should stimulate future research. The dataset together with code for the associated large-scale learning models, features, visualization tools, as well as the evaluation server, is available online at http://vision.imar.ro/human3.6m. ",,http://vision.imar.ro/human3.6m/pami-h36m.pdf,,,,,,
,PoseFormerV2: Exploring Frequency Domain for Efficient and Robust 3D Human Pose Estimation,Techinical ,"Recently, transformer-based methods have gained significant success in sequential 2D-to-3D lifting human pose estimation. As a pioneering work, PoseFormer captures spatial relations of human joints in each video frame and human dynamics across frames with cascaded transformer layers and has achieved impressive performance. However, in real scenarios, the performance of PoseFormer and its follow-ups is limited by two factors: (a) The length of the input joint sequence; (b) The quality of 2D joint detection. Existing methods typically apply self-attention to all frames of the input sequence, causing a huge computational burden when the frame number is increased to obtain advanced estimation accuracy, and they are not robust to noise naturally brought by the limited capability of 2D joint detectors. In this paper, we propose PoseFormerV2, which exploits a compact representation of lengthy skeleton sequences in the frequency domain to efficiently scale up the receptive field and boost robustness to noisy 2D joint detection. With minimum modifications to PoseFormer, the proposed method effectively fuses features both in the time domain and frequency domain, enjoying a better speed-accuracy trade-off than its precursor. Extensive experiments on two benchmark datasets (i.e., Human3.6M and MPI-INF-3DHP) demonstrate that the proposed approach significantly outperforms the original PoseFormer and other transformer-based variants. Code is released at https://github.com/ QitaoZhao/PoseFormerV2. ",,https://arxiv.org/pdf/2303.17472.pdf,,,,,,
,Review of Action Recognition and Detection Methods,Techinical ,"In computer vision, action recognition refers to the act of classifying an action that is present in a given video and action detection involves locating actions of interest in space and/or time. Videos, which contain photometric information (e.g. RGB, intensity values) in a lattice structure, contain information that can assist in identifying the action that has been imaged. The process of action recognition and detection often begins with extracting useful features and encoding them to ensure that the features are specific to serve the task of action recognition and detection. Encoded features are then processed through a classifier to identify the action class and their spatial and/or temporal locations. In this report, a thorough review of various action recognition and detection algorithms in computer vision is provided by analyzing the two-step process of a typical action recognition and detection algorithm: (i) extraction and encoding of features, and (ii) classifying features into action classes. In efforts to ensure that computer vision-based algorithms reach the capabilities that humans have of identifying actions irrespective of various nuisance variables that may be present within the field of view, the state-of-the-art methods are reviewed and some remaining problems are addressed in the final chapter.",,https://arxiv.org/abs/1610.06906,,,,,,
,"Action Recognition Datasets: ""NTU RGB+D"" Dataset and ""NTU RGB+D 120"" Dataset",Dataset,"This page introduces two datasets: ""NTU RGB+D"" and ""NTU RGB+D 120"".",,https://rose1.ntu.edu.sg/dataset/actionRecognition/,,,,,,
,,,"""NTU RGB+D"" contains 60 action classes and 56,880 video samples.",,,,,,,,
,,,"""NTU RGB+D 120"" extends ""NTU RGB+D"" by adding another 60 classes and another 57,600 video samples, i.e., ""NTU RGB+D 120"" has 120 classes and 114,480 samples in total.",,,,,,,,
,,,"These two datasets both contain RGB videos, depth map sequences, 3D skeletal data, and infrared (IR) videos for each sample. Each dataset is captured by three Kinect V2 cameras concurrently.",,,,,,,,
,,,"The resolutions of RGB videos are 1920x1080, depth maps and IR videos are all in 512x424, and 3D skeletal data contains the 3D coordinates of 25 body joints at each frame.",,,,,,,,
dora,Breakfast: A Large-Scale Database for Video Understanding,Dataset,"Breakfast dataset contains videos of people preparing breakfast in uncontrolled environments. Activities are defined and labeled based on cooking actions and interactions with kitchen objects. Annotations include timestamps for activity segments, with transitions between activities annotated.",,https://serre-lab.clps.brown.edu/resource/breakfast-actions-dataset/,"10 actions performed by 52 individuals in 18 kitchens, between 3-5 cameras per session, meant to capture realistic natural action rather than lab environment type, videos normalized to consistent FPS and pixels,",10 activities total:,"annotated as coarse actions and ""silence"" samples","doesn't look like it, actions saved as coarse actions and in between moments as ""silence"" in dataset",interesting idea to use speech recognition techniques applied to video data for activity recognition,
,,,,,https://serre-lab.clps.brown.edu/wp-content/uploads/2014/05/paper_cameraReady-2.pdf,"actors were not scripted or directed, just handed a recipe and told to prepare the listed food item(s)",preparation of:,coarse actions are made up of a series of finer actions that often take place in different orders for different participants,,,
,,,,,,,1. coffee,"""silence"" samples are the moments in between coarse actions",,,
,,,,,,,2. orange juice,,,,
,,,,,,,3. chocolate milk,,,,
,,,,,,,4. tea,,,,
,,,,,,,5. cereal,,,,
,,,,,,,6. fried eggs,,,,
,,,,,,,7. pancakes,,,,
,,,,,,,8. fruit salad,,,,
,,,,,,,9. sandwich,,,,
,,,,,,,10. scrambled eggs,,,,
dora,"The EPIC-KITCHENS Dataset: Collection, Challenges and Baselines",Dataset,"Provides a comprehensive overview of the EPIC-KITCHENS dataset, focusing on egocentric video understanding. Activities are annotated with both action labels and object interactions. Annotations include timestamps and temporal duration, aiding in understanding activity boundaries and transitions.",,https://epic-kitchens.github.io/2018,"first-person headcam recordings, non-scripted actors in their own kitchens, annotated using live audio feed, across 32 kitchens in 4 cities, actors wear headcam whenever they go to work in the kitchen","almost 40k action segments, massive number of actions defined, also defines objects and object interactions in addition to actions, defines actions with combos of verb and noun classes","activities are marked based on the audio commentary,",not discussed -- seems researchers kept all video data and annotated relevant clips with time stamps,"actions were annotated based on the narration of the actor, as they are the expert on what they are doing, but were quality checked by a team of annotators and then confirmed via random sampling and manual viewing by the research team",
,,,,,https://arxiv.org/pdf/1804.02748.pdf,,,"the actor will narrate their action (""i'm opening miso paste"") and the action is identified based on the key words given (open, paste)",,,
,,,,,,"""55hrs of recording, densely annotated",,,,,
,,,,,,"with start/end times for each action/interaction, as well as bounding boxes",,,,,
,,,,,,"around objects subject to interaction""",,,,,
iman,ActivityNet: A Large-Scale Video Benchmark for Human Activity Understanding,Dataset,"Activities are annotated with timestamps, indicating the start and end times of each activity instance. Activities in ActivityNet are defined and labeled based on a hierarchical structure with multiple levels of granularity. The dataset contains annotations for activities as well as ""in between"" segments, allowing for a more comprehensive understanding of video content.",,https://ieeexplore.ieee.org/document/7298698,"They heavily rely on the crowd and specifically, Amazon Mechanical Turk, to help acquire and annotate ActivityNet. Our acquisition pipeline has three main steps: Collection, Filtering, and Temporal Localization.","ActivityNet provides samples from 203 activity classes with an average of 137 untrimmed videos per class and 1.41 activity instances per video, for a total of 849 video hours. ",The videos are mually trimmed.,,,
iman,UCF101 ,Dataset,"We introduce UCF101 which is currently the largest dataset of human actions. It consists of 101 action classes, over 13k clips and 27 hours of video data. The database consists of realistic user-uploaded videos containing camera motion and cluttered background. Additionally, we provide baseline action recognition results on this new dataset using standard bag of words approach with overall performance of 43.9%. To the best of our knowledge, UCF101 is currently the most challenging dataset of actions due to its large number of classes, large number of clips and also unconstrained nature of such clips.",,https://www.crcv.ucf.edu/papers/UCF101_CRCV-TR-12-01.pdf,This benchmark dataset was collected from Youtube videos,"There are 101 action categories defined from 13320 videos. It is the largest in terms of diversity of actions.  The action categories for UCF101 data set are: Apply Eye Makeup, Apply Lipstick, Archery, Baby Crawling, Balance Beam, Band Marching, Baseball Pitch, Basketball Shooting, Basketball Dunk, Bench Press, Biking, Billiards Shot, Blow Dry Hair, Blowing Candles, Body Weight Squats, Bowling, Boxing Punching Bag, Boxing Speed Bag, Breaststroke, Brushing Teeth, Clean and Jerk, Cliff Diving, Cricket Bowling, Cricket Shot, Cutting In Kitchen, Diving, Drumming, Fencing, Field Hockey Penalty, Floor Gymnastics, Frisbee Catch, Front Crawl, Golf Swing, Haircut, Hammer Throw, Hammering, Handstand Pushups, Handstand Walking, Head Massage, High Jump, Horse Race, Horse Riding, Hula Hoop, Ice Dancing, Javelin Throw, Juggling Balls, Jump Rope, Jumping Jack, Kayaking, Knitting, Long Jump, Lunges, Military Parade, Mixing Batter, Mopping Floor, Nun chucks, Parallel Bars, Pizza Tossing, Playing Guitar, Playing Piano, Playing Tabla, Playing Violin, Playing Cello, Playing Daf, Playing Dhol, Playing Flute, Playing Sitar, Pole Vault, Pommel Horse, Pull Ups, Punch, Push Ups, Rafting, Rock Climbing Indoor, Rope Climbing, Rowing, Salsa Spins, Shaving Beard, Shotput, Skate Boarding, Skiing, Skijet, Sky Diving, Soccer Juggling, Soccer Penalty, Still Rings, Sumo Wrestling, Surfing, Swing, Table Tennis Shot, Tai Chi, Tennis Swing, Throw Discus, Trampoline Jumping, Typing, Uneven Bars, Volleyball Spiking, Walking with a dog, Wall Pushups, Writing On Board, Yo Yo.",They use a Sequential Bag of Words technique,The videos are downloaded from YouTube  and the,,
,,,,,,,,,irrelevant ones are manually removed. All clips have fixed,,
,,,,,,,,,frame rate and resolution of 25 FPS and 320 × 240 respectively. The videos are saved in .avi files compressed using DivX codec available in k-lite package [1]. The audio,,
,,,,,,,,,is preserved for the clips of the new 51 actions.,,
,MPII Cooking Activities,Dataset,"This dataset focuses on cooking activities, captured from egocentric viewpoints. Activities are defined and labeled based on cooking actions and object interactions. Annotations include timestamps for activity segments, with transitions between activities marked to delineate boundaries.",,,,,,,,
,The Kinetics Human Action Video Dataset,Dataset,"We describe the DeepMind Kinetics human action video dataset. The dataset contains 400 human action classes, with at least 400 video clips for each action. Each clip lasts around 10s and is taken from a different YouTube video. The actions are human focussed and cover a broad range of classes including human-object interactions such as playing instruments, as well as human-human interactions such as shaking hands. We describe the statistics of the dataset, how it was collected, and give some baseline performance figures for neural network architectures trained and tested for human action classification on this dataset. We also carry out a preliminary analysis of whether imbalance in the dataset leads to bias in the classifiers.",,https://arxiv.org/abs/1705.06950,,,,,,
,Epic-Kitchens: A Dataset for Egocentric Video Understanding,Dataset,"Epic-Kitchens dataset captures daily activities from the perspective of individuals wearing egocentric cameras. Activities are defined and labeled with action verbs and object interactions. Annotations include timestamps for activity instances, with detailed descriptions of each action performed.",,,,,,,,
siv,THUMOS Challenge: Action Recognition with a Large Number of Classes,Dataset,"Activities in the THUMOS dataset are defined and labeled with a large number of action classes. Annotations include timestamps for the start and end of each activity instance, allowing precise localization within videos. Criteria for defining activity boundaries are based on human judgment, considering temporal and spatial context within videos.",,https://www.crcv.ucf.edu/THUMOS14/home.html,Dataset has action recognition and temporal action detection tasks and contains 4 parts:,101 human action categories,"Activities are marked based on the presence or absence of specific action classes in test videos, alongside a confidence score.",No reference of data cleaning,defining evaluation metric for the tasks for precise task recognization,
,,,,,,Training Data,(https://www.crcv.ucf.edu/THUMOS14/Class%20Index.txt),"For the Action Recognition task, each test video's prediction involves outputting a real-valued score indicating the confidence of the predicted presence of the action class within the video. In contrast, for the",,,
,,,,,,Validation Data,,"Temporal Action Detection task, the prediction not only involves the presence of the action class but also its temporal localization, i.e., the starting and ending times of each detected instance. Ground truth annotations for the temporal locations of action instances are provided for validation videos evaluation. ",,,
,,,,,,Background data,,,,,
,,,,,,Test Data,,,,,
,,,,,,,,,,,
,,,,,,"The training data comprises the entire UCF101 action dataset for action recognition, encompassing 101 human action categories with 13,320 temporally trimmed videos, while for temporal action detection, a subset of the UCF101 action dataset with 20 action classes is utilized, with the option to incorporate the remaining action classes if necessary.",,,,,
,,,,,," In the validation phase, 1,000 videos are allocated for action recognition, each action class having precisely 10 videos, accompanied by video-level label information indicating primary and secondary actions, albeit all videos remain temporally untrimmed. For temporal action detection, 200 videos from 20 action classes serve as validation data, featuring temporal annotations detailing the start and end times of action instances.",,,,,
,,,,,," Background data, totaling 2,500 videos exclude instances of any of the 101 or 20 action classes, depending on the task, with each background video pertinent to an action class andthe primary class for reference.",,,,,
,,,,,,"Test data has 1,574 temporally untrimmed videos for both action and temporal recognization, which may include instances of one or multiple action classes or none at all.",,,,,
siv,Activity Recognition in the Home Using Simple and Ubiquitous Sensors,Dataset,"This paper presents a dataset constructed using simple and ubiquitous sensors deployed in home environments. Activities are defined and labeled based on sensor readings and human observations. Annotations include timestamps for activities as well as transitions between activities, providing detailed temporal information.",,https://courses.media.mit.edu/2004fall/mas622j/04.projects/home/TapiaIntilleLarson04.pdf,"Data collection methods includes video recordings of tutoring sessions, audio recordings, transcripts of verbal interactions, observational notes, surveys, and questionnaires to capture both qualitative and quantitative data about the tutoring context, learner behavior, and tutor behavior.",Tutoring Activities,Bayesian classifiers were used to detect activities using,No reference of data cleaning,Activity recognition system architecture:,
,,,,,,,Learning Activities,the tape-on sensor system,,The proposed system consists of three major components: (1) The environmental,
,,,,,,,,,,state-change sensors used to collect information about use of objects in the,
,,,,,,,,,,"environment, (2) the context-aware experience sampling tool (ESM) used by the",
,,,,,,,,,,"end user to label his or her own activities, and (3) the pattern recognition and",
,,,,,,,,,,classification algorithms for recognizing activities after constructing a model,
,,,,,,,,,,based on a training set.,
,AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Actions,Dataset,AVA dataset focuses on spatio-temporally localized atomic visual actions in videos. Activities are labeled with atomic action labels localized both spatially and temporally. Annotations provide precise temporal information about action instances.,,,,,,,,
suraj,Charades: A Large-Scale Dataset for Video Understanding,Dataset,"Charades dataset comprises real-world activities performed by actors in unscripted environments. Activities are defined and labeled with free-form natural language descriptions. Annotations include timestamps for activity start and end points, providing temporal localization within videos.",,https://arxiv.org/abs/1804.09626,Recruited crowd workers over the internet to self record 1st person and 3rd person actions with a given script. 80/20 split. 8.72 activities per video on avg.,Scripts come from the Charades dataset.,Script is provided to workers,not mentioned,"although actions are the same, 1st person and 3rd person actions are picked up differently.","For 1st person, users had option to hold the camera on their forehead or create a head mount. One arm vs Two arms. Latter option had a monetary incentive. $.50 bonus"
,Something-Else: Compositional Action Recognition with Spatial-Temporal Interaction Networks,Dataset,"Introduces the Something-Else dataset for compositional action recognition. Activities are defined based on the interactions between objects and human poses. Annotations include temporal localization of actions, focusing on spatial-temporal interactions.",,,,,,,,
suraj,Moments in Time Dataset: one million videos for event understanding,Dataset,"Moments in Time dataset provides a large-scale collection of videos for event understanding. Activities are annotated with temporal segments and descriptive labels. Annotations capture a diverse range of events, providing insights into everyday activities.",,https://arxiv.org/abs/1801.03150,"one million short videos each with a label corresponding, to an event unfolding in 3 seconds -- avg duration of human memory","opening various things, closing various things",Each video is tagged with one action or activity label among 339 different classes),background clutter mentioned but no cleaning,Compound activities that occur at longer time scales can be represented by sequences of three second actions (e.g.,
,,,,,,,,,,agent and scene),